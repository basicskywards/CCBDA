{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler, SQLTransformer\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import rank,sum,col\n",
    "from pyspark.ml.classification import RandomForestClassifier as RF\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "from pyspark.ml.classification import GBTClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUBLIC dataset here\n",
    "path = 'public.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List numerical features & categorical features\n",
    "target_col = \"Exited\"\n",
    "use_cols = ['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "cate_cols = [\"Geography\", 'Gender']\n",
    "all_col = use_cols.append(target_col)\n",
    "num_cols = list(set(use_cols) - set(cate_cols) - set([target_col]))\n",
    "#num_cols.append('weights')\n",
    "#print(num_cols)\n",
    "\n",
    "def load(path):\n",
    "    # Load DataFrame\n",
    "    #path = \"public.csv\"\n",
    "    #df = SQLContext.read.load(path)\n",
    "    # ----\n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"Churn_Modelling\").getOrCreate()\n",
    "    df = spark.read.csv(path,header=True,inferSchema=True)\n",
    "    # Select useful columns (drop columns that should not be known \n",
    "    # before the flight take place) \n",
    "    #df = df.select(all_col)\n",
    "    df = df.select('CustomerId', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited')\n",
    "\n",
    "    # Impute numerical features\n",
    "    for col in num_cols:\n",
    "        df = df.withColumn(col, df[col].cast('double'))\n",
    "        mu = df.select(col).agg({col:'mean'}).collect()[0][0]\n",
    "        df = df.withColumn(col, F.when(df[col].isNull(), mu)\\\n",
    "                           .otherwise(df[col]))\n",
    "        \n",
    "    df = df.withColumn('label', df[target_col].cast('double'))\n",
    "    df = df.filter(df['label'].isNotNull())\n",
    "\n",
    "    # Impute categorical features\n",
    "    for col in cate_cols:\n",
    "        frq = df.select(col).groupby(col).count()\\\n",
    "                            .orderBy('count', ascending=False) \\\n",
    "                            .limit(1).collect()[0][0]\n",
    "        df = df.withColumn(col, F.when((df[col].isNull() | \n",
    "                                       (df[col] == '')), frq) \\\n",
    "                                .otherwise(df[col]))\n",
    "\n",
    "    return df\n",
    "#path = 'public'\n",
    "df = load(path)  \n",
    "# adding the new column weights and fill it with ratios\n",
    "\n",
    "ratio = 0.92\n",
    "def weight_balance(labels):\n",
    "    return when(labels == 1, ratio).otherwise(1*(1-ratio))\n",
    "\n",
    "df = df.withColumn('weights', weight_balance(col('label')))\n",
    "num_cols.append('weights')\n",
    "\n",
    "def gen_preprocessor(df):\n",
    "    # String Indexing for categorical features\n",
    "    indexers = [StringIndexer(inputCol=col, \n",
    "                              outputCol=\"{}_idx\".format(col)) \\\n",
    "                              for col in cate_cols]\n",
    "    \n",
    "    # One-hot encoding for categorical features\n",
    "    encoders = [OneHotEncoder(inputCol=\"{}_idx\".format(col), \n",
    "                              outputCol=\"{}_oh\".format(col)) \\\n",
    "                              for col in cate_cols]\n",
    "\n",
    "    # Concat Feature Columns\n",
    "    assembler = VectorAssembler(inputCols = num_cols + \\\n",
    "                            [\"{}_oh\".format(col) for col in cate_cols], \n",
    "                            outputCol = \"_features\")\n",
    "    \n",
    "    # Standardize Features\n",
    "    scaler = StandardScaler(inputCol='_features', \n",
    "                            outputCol='features', \n",
    "                            withStd=True, withMean=False)\n",
    "\n",
    "    preprocessor = Pipeline(stages = indexers + encoders + \\\n",
    "                                     [assembler, scaler]).fit(df)\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "preprocessor = gen_preprocessor(df)   \n",
    "df = preprocessor.transform(df) \n",
    "\n",
    "def eval_f1(predictions):\n",
    "    labe = np.array(predictions.select('label').collect())\n",
    "    pred = np.array(predictions.select('prediction').collect())\n",
    "    print(\"f1-score: \", metrics.f1_score(labe, pred, average = 'micro'))\n",
    "\n",
    "# train/test\n",
    "df.cache()\n",
    "train_data, test_data = df.randomSplit([1.0, 0.0])\n",
    "\n",
    "# TRAINING\n",
    "## RF\n",
    "#rf = RF(labelCol = 'label', featuresCol = 'features', numTrees = 200)\n",
    "#rf_fit = rf.fit(train_data)\n",
    "#transformed = rf_fit.transform(test_data)\n",
    "#eval_f1(transformed)\n",
    "\n",
    "# GBT\n",
    "gbt = GBTClassifier(maxIter = 25)\n",
    "gbtModel = gbt.fit(train_data)\n",
    "#predictions3 = gbtModel.transform(test_data)\n",
    "#eval_f1(predictions3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load private dataset, the same structure as public dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input path for PRIVATE dataset here\n",
    "path = 'public.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List numerical features & categorical features\n",
    "target_col = \"Exited\"\n",
    "use_cols = ['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "cate_cols = [\"Geography\", 'Gender']\n",
    "all_col = use_cols.append(target_col)\n",
    "num_cols = list(set(use_cols) - set(cate_cols) - set([target_col]))\n",
    "\n",
    "# load private dataset\n",
    "df_private = load(path)  \n",
    "\n",
    "# adding the new column weights and fill it with ratios\n",
    "ratio = 0.92\n",
    "df_private = df_private.withColumn('weights', weight_balance(col('label')))\n",
    "num_cols.append('weights')\n",
    "\n",
    "# pre-processing private dataset\n",
    "preprocessor = gen_preprocessor(df_private)   \n",
    "df_private = preprocessor.transform(df_private) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do prediction with your PySpark model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# prediction private dataset\n",
    "gbt_predictions = gbtModel.transform(df_private)\n",
    "#rf_predictions = rf_fit.transform(df_private)\n",
    "\n",
    "# f1-score from Gradient Boost Tree\n",
    "eval_f1(gbt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Your result as the following type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|CustomerId|prediction|\n",
      "+----------+----------+\n",
      "|  15565701|       0.0|\n",
      "|  15565706|       1.0|\n",
      "|  15565796|       0.0|\n",
      "|  15565806|       0.0|\n",
      "|  15565878|       0.0|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validate result format\n",
    "gbt_predictions.select('CustomerId','prediction').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TA will use the following function to get your prediction result (f-1 score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For TA compute f1-score again\n",
    "predicted =  np.array(gbt_predictions.select('prediction').collect())\n",
    "label = np.array(gbt_predictions.select('label').collect())\n",
    "metrics.f1_score(predicted,label, average = 'micro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
